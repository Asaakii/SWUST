# **决策树**
## 1 综述
### 1.1 决策树
- 什么是决策树？
    - 决策树是一棵树。
    - 他是基于一系列if-then规则的一棵树，通过对数据集利用if-then的不断划分，将数据进行分类。
- 学习目标与本质
    - 目标：根据训练数据集构造一个决策模型。
    - 本质：从训练数据中归纳出一组分类规则。
    - 决策树学习的是由训练数据集估计条件概率模型。
- 损失函数
    - 决策树学习的损失函数通常是正则化的极大似然函数。
    - 学习策略是以损失函数为目标函数的最小化。
- 决策树构建
    - 特征选择
    - 决策树生成
    - 决策树修剪
- 决策树是一个非参数决策算法，既可以解决分类问题，支持天然多分类问题，也可以解决回归问题。每个叶子节点对应一个数值（落在叶子结点的所有样本的平均值，作为回归的预测结果）。

### 1.2 信息熵
定义：熵是热力学的概念，表示混乱程度。信息熵表示随机变量的不确定度。越随机、不确定性越高，信息熵越大，不确定性越低，信息熵越小。
香农公式：
```math
H=-\sum_{i=1}^{k}p_{i}log(p_{i})
其中：p_{i}表示该分类的概率（n/k）
```
如果在二分类情形，信息熵为：
```math
H=-\sum_{i=1}^{k}p_{i}log(p_{i})=-xlog(x)-(1-x)log(1-x)
```

### 1.3 条件熵
定义：条件熵H(Y|X)表示在已知随机变量X的条件下随机变量Y的不确定性。随机变量X给定的条件下随机变量Y的条件熵H(Y|X)定义为X给定条件下，Y的条件概率分布的熵对X的期望。
```math
H(Y|X)=\sum_{i=1}^{n}p_{i}H(Y|X=x_{i})\\
其中：p_{i}=P(X=x_{i}),i=1,2，...，n
```
### 1.4 信息增益
定义：划分数据集前后信息发生的变化，称为信息增益。获得信息增益最高的特征就是最好的选择。
```math
g(D,A)=H(D)-H(D|A)\\
其中：D(H)为划分前集合D的经验熵H(D);H(D|A)为条件熵。
```
为了提高信息增益，H(D|A)值越小越好，那么选择A特征使得划分后，数据集的随机性降低。信息增益偏向于取值较多的特征。
### 1.5 信息增益率
由于信息增益偏向于划分特征较多的情形。因此，如果选择身份证为特征，划分效果最好，但不合理。因此需要定义惩罚项。
```math
g_{R}(D,A)=\frac{g(D,A)}{H_{A}(D)}\\ 
其中H_{A}(D)为对于样本集合D，将当前特征A作为随机变量（取值是特征A的各个特征值），求的经验熵。
```
- **信息增益比的本质：** 在信息增益的基础上乘上惩罚参数。惩罚系数是以特征A作为随机变量的熵的倒数，即特征A取值相同的样本划分到同一个子集中。
- **信息增益比的缺点：** 偏向于取值较少的特征。因为当`$H_{A}(D)$`较小，倒数较大，因而信息增益比较大。
- **特征选择：** 在使用增益信息比时，并不是直接选择信息增益率最大的特征，而是现在候选特征中找出信息增益高于平均水平的特征，然后在这些特征中再选择信息增益率最高的特征。

### 1.6 基尼系数
- 定义：
```math
Gini(p)=\sum_{k=1}^{K}p_{k}(1-p_{k})=1-\sum_{k=1}^{K}p^{2}
```
- `$p_{k}$`：表示选中样本属于k类别的概率，则这个样本被分错的概率为（`$1-p_{k}$`）
- 因为样本集合中有k个类别，一个随机选中的样本可以属于这k个类别中的任意一个，因而累加所有的k个类别。
- 当二分类时，`$G=2p(1-p)$`。
若每个类别的概率为`$\frac{|C_{k}|}{|D|}$`,其中`$|C_{k}|$`表示k的样本个数，`$|D|$`表示样本总数。则有定义：
```math
Gini(p)=1-\sum_{k=1}^{K}(\frac{|C_{k}|}{|D|})^{2}
```
### 1.7 ID3
**ID3的构建可以看成使用贪心算法得到近似最优的一颗决策树。**
- 实现流程：
    - 从根结点(root node)开始，对结点计算所有可能的特征的信息增益，选择信息增益最大的特征作为结点的特征。
    - 由该特征的不同取值建立子节点，再对子结点递归地调用以上方法，构建决策树；直到所有特征的信息增益均很小或没有特征可以选择为止；
    - 最后得到一个决策树。
- 优点：
    - 决策树易于理解与实现。
    - 数据简单，可以不用预处理。
    - 能够处理数据型和常规型属性。
    - 白盒模型。
    - 易于通过静态模型测试来对模型进行评测。
    - 短时间内能处理大数据，且有一个相对还行的结果。
- 缺点：
    - 无剪枝，容易过拟合。
    - 信息增益的方法偏向选择具有大量值的属性，划分不合理。
    - 只可以处理离散分布的数据特征。

### 1.8 C4.5
- 相对ID3改进：
    - 用信息增益比来选择属性。
    - 在决策树的构造过程中对树进行剪枝。
    - 对非离散数据也能处理。
    - 能够对不完整数据进行处理。

### 1.8 分类与回归树CART
- CART分类树算法每次仅对某个特征的值进行二分，而不是多分，这样CART分类树算法建立起来的是二叉树，而不是多叉树。
- 离散特征处理：CART采用的是不停的二分。会考虑把特征A分成{A1}和{A2,A3}、{A2}和{A1,A3}、{A3}和{A1,A2}三种情况，找到基尼系数最小的组合。
- 连续特征处理：CART分类树算法对连续值的处理，思想和C4.5相同，都是将连续的特征离散化。
    - 流程：CART取相邻两样本值的平均数做划分点，一共取m-1个，分别求得基尼系数，选择最小值进行划分。从而将连续数据离散化。
- CART回归树和CART分类树区别：
    - 如果样本输出是离散值，这是分类树;
    - 样本输出是连续值，这是回归树;
    - 分类树的输出是样本的类别;
    - 回归树的输出是一个实数;
    - 回归树采用最小化均方差和进行最优划分特征的选择（回归树输出不是类别，采用叶子节点的均值或者中位数来预测输出结果。）;
    - 分类树采用基尼系数的大小度量特征各个划分点的优劣。
- CART剪枝（后剪枝法）：
    - 从原始决策树生成各种剪枝效果的决策树；
    - 用交叉验证来检验剪枝后的预测能力，选择泛化预测能力最好的剪枝后的树作为最终的CART树。


### 1.9 剪枝
- 常用剪枝操作：
    - 预剪枝
        - 对每个节点在划分前先进行估计，通过预留的验证集判断当前节点能否提升决策树的能力。若当前节点的划分不能带来决策树泛化性能的提升，则停止划分并将当前节点标记为叶节点。
    - 后剪枝
        - 后剪枝是先从训练集生成一颗完整的决策树，然后自底向上地对非叶节点进行考察，若将该节点对应的子树完全替换为叶节点能带来决策树繁花性的提升，则将该子树替换为叶节点。
    - 对比：
        - 后剪枝决策树的欠拟合风险小。
        - 泛化性能往往也要优于预剪枝决策树。
## 2 代码实现
### 2.1 决策树函数
```
class sklearn.tree.DecisionTreeClassifier(criterion=’gini’, splitter=’best’, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, class_weight=None, presort=False)[source]
```
参数说明：
- criterion：特征选择标准，可选参数，默认是gini（基尼系数），可以设置为entropy（香农熵）。
- splitter：特征划分点选择标准，可选参数，默认是best，可以设置为random。默认的”best”适合样本量不大的时候，而如果样本数据量非常大，此时决策树构建推荐”random”。
- max_features：划分时考虑的最大特征数，可选参数，默认是None。寻找最佳切分时考虑的最大特征数(n_features为总共的特征数)
    - 如果max_features是整型的数，则考虑max_features个特征；
    - 如果max_features是浮点型的数，则考虑int(max_features * n_features)个特征；
    - 如果max_features设为auto，那么max_features = sqrt(n_features)；
    - 如果max_features设为sqrt，那么max_featrues = sqrt(n_features)，跟auto一样；
    - 如果max_features设为log2，那么max_features = log2(n_features)；
    - 如果max_features设为None，那么max_features = n_features，也就是所有特征都用。如果特征小于50，则选择None即可。
- max_depth：决策树最大深，可选参数，默认是None。常用的可以取值10-100之间。
- min_samples_split：内部节点再划分所需最小样本数。
- min_weight_fraction_leaf：叶子节点最小的样本权重和，可选参数，默认是0。
- max_leaf_nodes：最大叶子节点数，可选参数，默认是None。
- class_weight：类别权重，可选参数，默认是None，也可以字典、字典列表、balanced。防止训练集某些类别的样本过多，导致训练的决策树过于偏向这些类别。
- random_state：可选参数，默认是None。随机数种子。
- min_impurity_split：节点划分最小不纯度,可选参数，默认是1e-7。这是个阈值。
- presort：数据是否预排序，可选参数，默认为False。样本少时可以设置为True，可以提升速度；样本多时，没好处。

调参注意点：
1. 样本少、特征多，先做降纬处理（主成分分析（PCA），特征选择（Losso）或者独立成分分析（ICA）），防止过拟合。
2. 推荐多用决策树的可视化。容易判断是否过拟合。
3. 注意样本分布是否不均衡，如果是，则使用class_weight来限制模型过于偏向样本多的类别。
4. 如果输入的样本矩阵是稀疏的，推荐在拟合前调用csc_matrix稀疏化，在预测前调用csr_matrix稀疏化。
### 2.2 代码文件
文件名 | 描述 
:-:|:-:
01_Tree_demo.py|决策树可视化Demo
02_cal_entropy.py|计算交叉熵
03_all_entropy.py|各种划分计算方法
04_ID3.py|ID3算法实现
05_cut_leaf.py|剪枝实现
06_CART.py|CART实现

## 3 参考链接
决策树1：初识决策树：https://mp.weixin.qq.com/s?__biz=MzI4MjkzNTUxMw==&mid=2247484178&idx=1&sn=1af90ba53ee3c1036ca94be9180e4c81&chksm=eb932aa4dce4a3b23aaed46d779abbc00917a55ac71cf177737c462ef9a02732e087049d132a&scene=21#wechat_redirect\
决策树2: 特征选择中的相关概念：https://mp.weixin.qq.com/s?__biz=MzI4MjkzNTUxMw==&mid=2247484195&idx=1&sn=054651159e08f74c85f1fc1fab8a0f25&chksm=eb932a95dce4a383870f5385e9e926af022ecb5f7c9b827c2fb4c72af4f66a1133b5a88d3cb2&scene=21#wechat_redirect\
决策树3: 特征选择之寻找最优划分：https://mp.weixin.qq.com/s?__biz=MzI4MjkzNTUxMw==&mid=2247484204&idx=1&sn=372d3ee90802d15347445f91056fe6bc&chksm=eb932a9adce4a38c9750a2cf6ca3382056b66099476223da02016e964ef711b97c4798ba9e58&scene=21#wechat_redirect\
决策树4：构建算法之ID3、C4.5：https://mp.weixin.qq.com/s?__biz=MzI4MjkzNTUxMw==&mid=2247484212&idx=1&sn=aaabacf77db62a2b78fbef5ad034e354&chksm=eb932a82dce4a394fbf447fdd01b167565f04ddf85db5ef5e969bc0c75bd0dbf67b36236d64b&scene=21#wechat_redirect\
决策树5：剪枝与sklearn中的决策树：https://mp.weixin.qq.com/s?__biz=MzI4MjkzNTUxMw==&mid=2247484241&idx=1&sn=fa915fb55d98b38c54674a58f18ba921&chksm=eb932ae7dce4a3f16423bc2e61ca5f819b482e438df3025bcac6dd9d9c07ccf6424fc79463a9&scene=21#wechat_redirect\
决策树6：分类与回归树CART：https://mp.weixin.qq.com/s?__biz=MzI4MjkzNTUxMw==&mid=2247484269&idx=1&sn=a0c39116207b0d299c48764640d4c582&chksm=eb932adbdce4a3cd0a962b735ffb3fe5bad1739259ced35f3d1e03b8eee05db5cd9fb025395c&scene=21#wechat_redirect